{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "664pHAuC5vR4",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1tk-hGUVytDUfMSBuw8V8K6UnWZ_yY1ab\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boTqjD3P6qlt",
        "colab_type": "code",
        "outputId": "02aae45f-779a-4358-f8dd-ff19b8e24d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwMwmv2j62Jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp drive/'My Drive'/wonderland.txt ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHeC2ASy7uOQ",
        "colab_type": "code",
        "outputId": "f8dd4b70-1c28-49f6-ca09-73f56bcc375b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68KjSipm7-l7",
        "colab_type": "code",
        "outputId": "e8317780-82de-4025-a92b-158e75b2c970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "raw_text[:1000]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf1348\\\\cocoasubrtf170\\n{\\\\fonttbl\\\\f0\\\\fmodern\\\\fcharset0 courier;}\\n{\\\\colortbl;\\\\red255\\\\green255\\\\blue255;\\\\red0\\\\green0\\\\blue0;}\\n\\\\margl1440\\\\margr1440\\\\vieww10800\\\\viewh8400\\\\viewkind0\\n\\\\deftab720\\n\\\\pard\\\\pardeftab720\\n\\n\\\\f0\\\\fs24 \\\\cf2 \\\\expnd0\\\\expndtw0\\\\kerning0\\n\\\\outl0\\\\strokewidth0 \\\\strokec2 project gutenberg's alice's adventures in wonderland, by lewis carroll\\\\\\n\\\\\\nthis ebook is for the use of anyone anywhere at no cost and with\\\\\\nalmost no restrictions whatsoever.  you may copy it, give it away or\\\\\\nre-use it under the terms of the project gutenberg license included\\\\\\nwith this ebook or online at www.gutenberg.org\\\\\\n\\\\\\n\\\\\\ntitle: alice's adventures in wonderland\\\\\\n\\\\\\nauthor: lewis carroll\\\\\\n\\\\\\nposting date: june 25, 2008 [ebook #11]\\\\\\nrelease date: march, 1994\\\\\\n[last updated: december 20, 2011]\\\\\\n\\\\\\nlanguage: english\\\\\\n\\\\\\n\\\\\\n*** start of this project gutenberg ebook alice's adventures in wonderland ***\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\n\\\\\\nalice's adventures in wonderland\\\\\\n\\\\\\nlewis carroll\\\\\\n\\\\\\nthe millennium fu\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zozRTjEoNqKZ",
        "colab_type": "text"
      },
      "source": [
        "Removed Punctuation from source text for better results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rVNZgn18D5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "raw_text = re.sub(r'[^\\w\\s\\t\\n]','',raw_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0Q-DV3J8PRE",
        "colab_type": "code",
        "outputId": "a289636f-d03e-4ead-f170-06ea78515312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "chars"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wynE3v8l6dLN",
        "colab_type": "code",
        "outputId": "53565ed6-5adf-4222-e782-d90f8fbeb560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  155125\n",
            "Total Vocab:  39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDNI9nxoF_NV",
        "colab_type": "text"
      },
      "source": [
        "**Article is split by sentences and each set of 100 characters in a sentence is used for training. <br>\n",
        "If the length of sentence is less than 100 it is padded using keras pad_sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c1IVYOb90nA",
        "colab_type": "code",
        "outputId": "69ce4caf-8f43-45c3-b8f2-4425ebe5c117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sentence = raw_text.split(\"\\n\\n\")\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "sent = ([x for x in sentence if sum(c.isalpha() for c in x)>1])\n",
        "dataX = []\n",
        "dataY = []\n",
        "seq_length = 101\n",
        "for s in sent:\n",
        "  if len(s)<101:\n",
        "    x = [char_to_int[a] for a in s]\n",
        "    x = pad_sequences([x], maxlen=seq_length)\n",
        "    dataX.append([a for a in x[0][:-1]])\n",
        "    dataY.append(x[0][-1])\n",
        "  else:\n",
        "    for i in range(0, len(s) - seq_length, 1):\n",
        "      x = [char_to_int[a] for a in s[i:i + seq_length]] \n",
        "      dataX.append([a for a in x[:-1]])\n",
        "      dataY.append(x[-1])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "print(len(dataX))\n",
        "X = numpy.reshape(numpy.array(dataX), (n_patterns, seq_length-1, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)      "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  82392\n",
            "82392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGeEgohz8iEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aJMW2sh8kqE",
        "colab_type": "code",
        "outputId": "df9c8928-1185-4341-9227-73f2a7f0cf45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "82392/82392 [==============================] - 90s 1ms/step - loss: 2.8950\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.89498, saving model to weights-improvement-01-2.8950-bigger.hdf5\n",
            "Epoch 2/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 2.7933\n",
            "\n",
            "Epoch 00002: loss improved from 2.89498 to 2.79331, saving model to weights-improvement-02-2.7933-bigger.hdf5\n",
            "Epoch 3/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 2.7132\n",
            "\n",
            "Epoch 00003: loss improved from 2.79331 to 2.71321, saving model to weights-improvement-03-2.7132-bigger.hdf5\n",
            "Epoch 4/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 2.6598\n",
            "\n",
            "Epoch 00004: loss improved from 2.71321 to 2.65978, saving model to weights-improvement-04-2.6598-bigger.hdf5\n",
            "Epoch 5/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 2.6083\n",
            "\n",
            "Epoch 00005: loss improved from 2.65978 to 2.60831, saving model to weights-improvement-05-2.6083-bigger.hdf5\n",
            "Epoch 6/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 2.5499\n",
            "\n",
            "Epoch 00006: loss improved from 2.60831 to 2.54987, saving model to weights-improvement-06-2.5499-bigger.hdf5\n",
            "Epoch 7/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 2.4877\n",
            "\n",
            "Epoch 00007: loss improved from 2.54987 to 2.48769, saving model to weights-improvement-07-2.4877-bigger.hdf5\n",
            "Epoch 8/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 2.4352\n",
            "\n",
            "Epoch 00008: loss improved from 2.48769 to 2.43520, saving model to weights-improvement-08-2.4352-bigger.hdf5\n",
            "Epoch 9/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.3823\n",
            "\n",
            "Epoch 00009: loss improved from 2.43520 to 2.38232, saving model to weights-improvement-09-2.3823-bigger.hdf5\n",
            "Epoch 10/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.3434\n",
            "\n",
            "Epoch 00010: loss improved from 2.38232 to 2.34337, saving model to weights-improvement-10-2.3434-bigger.hdf5\n",
            "Epoch 11/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.3104\n",
            "\n",
            "Epoch 00011: loss improved from 2.34337 to 2.31041, saving model to weights-improvement-11-2.3104-bigger.hdf5\n",
            "Epoch 12/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 2.2764\n",
            "\n",
            "Epoch 00012: loss improved from 2.31041 to 2.27642, saving model to weights-improvement-12-2.2764-bigger.hdf5\n",
            "Epoch 13/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.2422\n",
            "\n",
            "Epoch 00013: loss improved from 2.27642 to 2.24223, saving model to weights-improvement-13-2.2422-bigger.hdf5\n",
            "Epoch 14/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.2197\n",
            "\n",
            "Epoch 00014: loss improved from 2.24223 to 2.21969, saving model to weights-improvement-14-2.2197-bigger.hdf5\n",
            "Epoch 15/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.1927\n",
            "\n",
            "Epoch 00015: loss improved from 2.21969 to 2.19271, saving model to weights-improvement-15-2.1927-bigger.hdf5\n",
            "Epoch 16/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.1681\n",
            "\n",
            "Epoch 00016: loss improved from 2.19271 to 2.16808, saving model to weights-improvement-16-2.1681-bigger.hdf5\n",
            "Epoch 17/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.1404\n",
            "\n",
            "Epoch 00017: loss improved from 2.16808 to 2.14037, saving model to weights-improvement-17-2.1404-bigger.hdf5\n",
            "Epoch 18/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.1196\n",
            "\n",
            "Epoch 00018: loss improved from 2.14037 to 2.11963, saving model to weights-improvement-18-2.1196-bigger.hdf5\n",
            "Epoch 19/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 2.1053\n",
            "\n",
            "Epoch 00019: loss improved from 2.11963 to 2.10530, saving model to weights-improvement-19-2.1053-bigger.hdf5\n",
            "Epoch 20/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.0790\n",
            "\n",
            "Epoch 00020: loss improved from 2.10530 to 2.07901, saving model to weights-improvement-20-2.0790-bigger.hdf5\n",
            "Epoch 21/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.0560\n",
            "\n",
            "Epoch 00021: loss improved from 2.07901 to 2.05598, saving model to weights-improvement-21-2.0560-bigger.hdf5\n",
            "Epoch 22/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.0367\n",
            "\n",
            "Epoch 00022: loss improved from 2.05598 to 2.03670, saving model to weights-improvement-22-2.0367-bigger.hdf5\n",
            "Epoch 23/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.0197\n",
            "\n",
            "Epoch 00023: loss improved from 2.03670 to 2.01974, saving model to weights-improvement-23-2.0197-bigger.hdf5\n",
            "Epoch 24/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 2.0008\n",
            "\n",
            "Epoch 00024: loss improved from 2.01974 to 2.00078, saving model to weights-improvement-24-2.0008-bigger.hdf5\n",
            "Epoch 25/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.9817\n",
            "\n",
            "Epoch 00025: loss improved from 2.00078 to 1.98169, saving model to weights-improvement-25-1.9817-bigger.hdf5\n",
            "Epoch 26/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.9716\n",
            "\n",
            "Epoch 00026: loss improved from 1.98169 to 1.97155, saving model to weights-improvement-26-1.9716-bigger.hdf5\n",
            "Epoch 27/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.9531\n",
            "\n",
            "Epoch 00027: loss improved from 1.97155 to 1.95311, saving model to weights-improvement-27-1.9531-bigger.hdf5\n",
            "Epoch 28/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.9444\n",
            "\n",
            "Epoch 00028: loss improved from 1.95311 to 1.94436, saving model to weights-improvement-28-1.9444-bigger.hdf5\n",
            "Epoch 29/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.9276\n",
            "\n",
            "Epoch 00029: loss improved from 1.94436 to 1.92758, saving model to weights-improvement-29-1.9276-bigger.hdf5\n",
            "Epoch 30/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.9123\n",
            "\n",
            "Epoch 00030: loss improved from 1.92758 to 1.91225, saving model to weights-improvement-30-1.9123-bigger.hdf5\n",
            "Epoch 31/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.9021\n",
            "\n",
            "Epoch 00031: loss improved from 1.91225 to 1.90207, saving model to weights-improvement-31-1.9021-bigger.hdf5\n",
            "Epoch 32/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 1.8889\n",
            "\n",
            "Epoch 00032: loss improved from 1.90207 to 1.88885, saving model to weights-improvement-32-1.8889-bigger.hdf5\n",
            "Epoch 33/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.8748\n",
            "\n",
            "Epoch 00033: loss improved from 1.88885 to 1.87485, saving model to weights-improvement-33-1.8748-bigger.hdf5\n",
            "Epoch 34/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.8594\n",
            "\n",
            "Epoch 00034: loss improved from 1.87485 to 1.85937, saving model to weights-improvement-34-1.8594-bigger.hdf5\n",
            "Epoch 35/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.8523\n",
            "\n",
            "Epoch 00035: loss improved from 1.85937 to 1.85228, saving model to weights-improvement-35-1.8523-bigger.hdf5\n",
            "Epoch 36/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.8439\n",
            "\n",
            "Epoch 00036: loss improved from 1.85228 to 1.84390, saving model to weights-improvement-36-1.8439-bigger.hdf5\n",
            "Epoch 37/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.8292\n",
            "\n",
            "Epoch 00037: loss improved from 1.84390 to 1.82918, saving model to weights-improvement-37-1.8292-bigger.hdf5\n",
            "Epoch 38/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.8132\n",
            "\n",
            "Epoch 00038: loss improved from 1.82918 to 1.81317, saving model to weights-improvement-38-1.8132-bigger.hdf5\n",
            "Epoch 39/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.8101\n",
            "\n",
            "Epoch 00039: loss improved from 1.81317 to 1.81007, saving model to weights-improvement-39-1.8101-bigger.hdf5\n",
            "Epoch 40/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.7995\n",
            "\n",
            "Epoch 00040: loss improved from 1.81007 to 1.79945, saving model to weights-improvement-40-1.7995-bigger.hdf5\n",
            "Epoch 41/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.7844\n",
            "\n",
            "Epoch 00041: loss improved from 1.79945 to 1.78438, saving model to weights-improvement-41-1.7844-bigger.hdf5\n",
            "Epoch 42/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.7831\n",
            "\n",
            "Epoch 00042: loss improved from 1.78438 to 1.78312, saving model to weights-improvement-42-1.7831-bigger.hdf5\n",
            "Epoch 43/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.7672\n",
            "\n",
            "Epoch 00043: loss improved from 1.78312 to 1.76720, saving model to weights-improvement-43-1.7672-bigger.hdf5\n",
            "Epoch 44/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.7583\n",
            "\n",
            "Epoch 00044: loss improved from 1.76720 to 1.75826, saving model to weights-improvement-44-1.7583-bigger.hdf5\n",
            "Epoch 45/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.7500\n",
            "\n",
            "Epoch 00045: loss improved from 1.75826 to 1.75005, saving model to weights-improvement-45-1.7500-bigger.hdf5\n",
            "Epoch 46/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.7409\n",
            "\n",
            "Epoch 00046: loss improved from 1.75005 to 1.74089, saving model to weights-improvement-46-1.7409-bigger.hdf5\n",
            "Epoch 47/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.7281\n",
            "\n",
            "Epoch 00047: loss improved from 1.74089 to 1.72806, saving model to weights-improvement-47-1.7281-bigger.hdf5\n",
            "Epoch 48/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.7273\n",
            "\n",
            "Epoch 00048: loss improved from 1.72806 to 1.72735, saving model to weights-improvement-48-1.7273-bigger.hdf5\n",
            "Epoch 49/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 1.7176\n",
            "\n",
            "Epoch 00049: loss improved from 1.72735 to 1.71760, saving model to weights-improvement-49-1.7176-bigger.hdf5\n",
            "Epoch 50/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.7054\n",
            "\n",
            "Epoch 00050: loss improved from 1.71760 to 1.70541, saving model to weights-improvement-50-1.7054-bigger.hdf5\n",
            "Epoch 51/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 1.6979\n",
            "\n",
            "Epoch 00051: loss improved from 1.70541 to 1.69790, saving model to weights-improvement-51-1.6979-bigger.hdf5\n",
            "Epoch 52/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 1.6929\n",
            "\n",
            "Epoch 00052: loss improved from 1.69790 to 1.69289, saving model to weights-improvement-52-1.6929-bigger.hdf5\n",
            "Epoch 53/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 1.6812\n",
            "\n",
            "Epoch 00053: loss improved from 1.69289 to 1.68119, saving model to weights-improvement-53-1.6812-bigger.hdf5\n",
            "Epoch 54/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.6747\n",
            "\n",
            "Epoch 00054: loss improved from 1.68119 to 1.67467, saving model to weights-improvement-54-1.6747-bigger.hdf5\n",
            "Epoch 55/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.6656\n",
            "\n",
            "Epoch 00055: loss improved from 1.67467 to 1.66565, saving model to weights-improvement-55-1.6656-bigger.hdf5\n",
            "Epoch 56/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 1.6583\n",
            "\n",
            "Epoch 00056: loss improved from 1.66565 to 1.65830, saving model to weights-improvement-56-1.6583-bigger.hdf5\n",
            "Epoch 57/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.6597\n",
            "\n",
            "Epoch 00057: loss did not improve from 1.65830\n",
            "Epoch 58/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 1.6433\n",
            "\n",
            "Epoch 00058: loss improved from 1.65830 to 1.64326, saving model to weights-improvement-58-1.6433-bigger.hdf5\n",
            "Epoch 59/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 1.6433\n",
            "\n",
            "Epoch 00059: loss did not improve from 1.64326\n",
            "Epoch 60/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 1.6323\n",
            "\n",
            "Epoch 00060: loss improved from 1.64326 to 1.63229, saving model to weights-improvement-60-1.6323-bigger.hdf5\n",
            "Epoch 61/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.6182\n",
            "\n",
            "Epoch 00061: loss improved from 1.63229 to 1.61821, saving model to weights-improvement-61-1.6182-bigger.hdf5\n",
            "Epoch 62/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 1.6178\n",
            "\n",
            "Epoch 00062: loss improved from 1.61821 to 1.61777, saving model to weights-improvement-62-1.6178-bigger.hdf5\n",
            "Epoch 63/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 1.6079\n",
            "\n",
            "Epoch 00063: loss improved from 1.61777 to 1.60792, saving model to weights-improvement-63-1.6079-bigger.hdf5\n",
            "Epoch 64/100\n",
            "82392/82392 [==============================] - 89s 1ms/step - loss: 1.6106\n",
            "\n",
            "Epoch 00064: loss did not improve from 1.60792\n",
            "Epoch 65/100\n",
            "82392/82392 [==============================] - 88s 1ms/step - loss: 1.5918\n",
            "\n",
            "Epoch 00065: loss improved from 1.60792 to 1.59181, saving model to weights-improvement-65-1.5918-bigger.hdf5\n",
            "Epoch 66/100\n",
            "82392/82392 [==============================] - 86s 1ms/step - loss: 1.5944\n",
            "\n",
            "Epoch 00066: loss did not improve from 1.59181\n",
            "Epoch 67/100\n",
            "82392/82392 [==============================] - 85s 1ms/step - loss: 1.5825\n",
            "\n",
            "Epoch 00067: loss improved from 1.59181 to 1.58254, saving model to weights-improvement-67-1.5825-bigger.hdf5\n",
            "Epoch 68/100\n",
            "82392/82392 [==============================] - 84s 1ms/step - loss: 1.5794\n",
            "\n",
            "Epoch 00068: loss improved from 1.58254 to 1.57944, saving model to weights-improvement-68-1.5794-bigger.hdf5\n",
            "Epoch 69/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.5729\n",
            "\n",
            "Epoch 00069: loss improved from 1.57944 to 1.57290, saving model to weights-improvement-69-1.5729-bigger.hdf5\n",
            "Epoch 70/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.5613\n",
            "\n",
            "Epoch 00070: loss improved from 1.57290 to 1.56133, saving model to weights-improvement-70-1.5613-bigger.hdf5\n",
            "Epoch 71/100\n",
            "82392/82392 [==============================] - 84s 1ms/step - loss: 1.5614\n",
            "\n",
            "Epoch 00071: loss did not improve from 1.56133\n",
            "Epoch 72/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.5585\n",
            "\n",
            "Epoch 00072: loss improved from 1.56133 to 1.55851, saving model to weights-improvement-72-1.5585-bigger.hdf5\n",
            "Epoch 73/100\n",
            "82392/82392 [==============================] - 81s 989us/step - loss: 1.5475\n",
            "\n",
            "Epoch 00073: loss improved from 1.55851 to 1.54748, saving model to weights-improvement-73-1.5475-bigger.hdf5\n",
            "Epoch 74/100\n",
            "82392/82392 [==============================] - 81s 988us/step - loss: 1.5440\n",
            "\n",
            "Epoch 00074: loss improved from 1.54748 to 1.54399, saving model to weights-improvement-74-1.5440-bigger.hdf5\n",
            "Epoch 75/100\n",
            "82392/82392 [==============================] - 82s 992us/step - loss: 1.5307\n",
            "\n",
            "Epoch 00075: loss improved from 1.54399 to 1.53071, saving model to weights-improvement-75-1.5307-bigger.hdf5\n",
            "Epoch 76/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.5337\n",
            "\n",
            "Epoch 00076: loss did not improve from 1.53071\n",
            "Epoch 77/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.5276\n",
            "\n",
            "Epoch 00077: loss improved from 1.53071 to 1.52760, saving model to weights-improvement-77-1.5276-bigger.hdf5\n",
            "Epoch 78/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.5178\n",
            "\n",
            "Epoch 00078: loss improved from 1.52760 to 1.51783, saving model to weights-improvement-78-1.5178-bigger.hdf5\n",
            "Epoch 79/100\n",
            "82392/82392 [==============================] - 85s 1ms/step - loss: 1.5214\n",
            "\n",
            "Epoch 00079: loss did not improve from 1.51783\n",
            "Epoch 80/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.5093\n",
            "\n",
            "Epoch 00080: loss improved from 1.51783 to 1.50931, saving model to weights-improvement-80-1.5093-bigger.hdf5\n",
            "Epoch 81/100\n",
            "82392/82392 [==============================] - 84s 1ms/step - loss: 1.5064\n",
            "\n",
            "Epoch 00081: loss improved from 1.50931 to 1.50644, saving model to weights-improvement-81-1.5064-bigger.hdf5\n",
            "Epoch 82/100\n",
            "82392/82392 [==============================] - 84s 1ms/step - loss: 1.4955\n",
            "\n",
            "Epoch 00082: loss improved from 1.50644 to 1.49551, saving model to weights-improvement-82-1.4955-bigger.hdf5\n",
            "Epoch 83/100\n",
            "82392/82392 [==============================] - 84s 1ms/step - loss: 1.4932\n",
            "\n",
            "Epoch 00083: loss improved from 1.49551 to 1.49322, saving model to weights-improvement-83-1.4932-bigger.hdf5\n",
            "Epoch 84/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.4870\n",
            "\n",
            "Epoch 00084: loss improved from 1.49322 to 1.48695, saving model to weights-improvement-84-1.4870-bigger.hdf5\n",
            "Epoch 85/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.4872\n",
            "\n",
            "Epoch 00085: loss did not improve from 1.48695\n",
            "Epoch 86/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.4796\n",
            "\n",
            "Epoch 00086: loss improved from 1.48695 to 1.47961, saving model to weights-improvement-86-1.4796-bigger.hdf5\n",
            "Epoch 87/100\n",
            "82392/82392 [==============================] - 82s 1000us/step - loss: 1.4777\n",
            "\n",
            "Epoch 00087: loss improved from 1.47961 to 1.47773, saving model to weights-improvement-87-1.4777-bigger.hdf5\n",
            "Epoch 88/100\n",
            "82392/82392 [==============================] - 82s 995us/step - loss: 1.4675\n",
            "\n",
            "Epoch 00088: loss improved from 1.47773 to 1.46751, saving model to weights-improvement-88-1.4675-bigger.hdf5\n",
            "Epoch 89/100\n",
            "82392/82392 [==============================] - 82s 994us/step - loss: 1.4646\n",
            "\n",
            "Epoch 00089: loss improved from 1.46751 to 1.46458, saving model to weights-improvement-89-1.4646-bigger.hdf5\n",
            "Epoch 90/100\n",
            "82392/82392 [==============================] - 82s 1ms/step - loss: 1.4615\n",
            "\n",
            "Epoch 00090: loss improved from 1.46458 to 1.46150, saving model to weights-improvement-90-1.4615-bigger.hdf5\n",
            "Epoch 91/100\n",
            "82392/82392 [==============================] - 82s 994us/step - loss: 1.4557\n",
            "\n",
            "Epoch 00091: loss improved from 1.46150 to 1.45573, saving model to weights-improvement-91-1.4557-bigger.hdf5\n",
            "Epoch 92/100\n",
            "82392/82392 [==============================] - 82s 993us/step - loss: 1.4529\n",
            "\n",
            "Epoch 00092: loss improved from 1.45573 to 1.45290, saving model to weights-improvement-92-1.4529-bigger.hdf5\n",
            "Epoch 93/100\n",
            "82392/82392 [==============================] - 82s 996us/step - loss: 1.4474\n",
            "\n",
            "Epoch 00093: loss improved from 1.45290 to 1.44741, saving model to weights-improvement-93-1.4474-bigger.hdf5\n",
            "Epoch 94/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.4397\n",
            "\n",
            "Epoch 00094: loss improved from 1.44741 to 1.43970, saving model to weights-improvement-94-1.4397-bigger.hdf5\n",
            "Epoch 95/100\n",
            "82392/82392 [==============================] - 82s 999us/step - loss: 1.4339\n",
            "\n",
            "Epoch 00095: loss improved from 1.43970 to 1.43387, saving model to weights-improvement-95-1.4339-bigger.hdf5\n",
            "Epoch 96/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.4304\n",
            "\n",
            "Epoch 00096: loss improved from 1.43387 to 1.43041, saving model to weights-improvement-96-1.4304-bigger.hdf5\n",
            "Epoch 97/100\n",
            "82392/82392 [==============================] - 82s 996us/step - loss: 1.4212\n",
            "\n",
            "Epoch 00097: loss improved from 1.43041 to 1.42119, saving model to weights-improvement-97-1.4212-bigger.hdf5\n",
            "Epoch 98/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.4214\n",
            "\n",
            "Epoch 00098: loss did not improve from 1.42119\n",
            "Epoch 99/100\n",
            "82392/82392 [==============================] - 83s 1ms/step - loss: 1.4162\n",
            "\n",
            "Epoch 00099: loss improved from 1.42119 to 1.41616, saving model to weights-improvement-99-1.4162-bigger.hdf5\n",
            "Epoch 100/100\n",
            "82392/82392 [==============================] - 82s 999us/step - loss: 1.4186\n",
            "\n",
            "Epoch 00100: loss did not improve from 1.41616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd6d014ee10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRd4RakZ8tPm",
        "colab_type": "code",
        "outputId": "d1163957-8dd1-4574-9394-970321a82019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import sys\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" hing but outoftheway\n",
            "things to happen that it seemed quite dull and stupid for life to go on\n",
            "in the  \"\n",
            "inanlny anlces neepinert tuojthing a pots anl tayaii ani eoy wet snlce ayayioy anl toecsned foisaclliny on tuesene  t teragling at  a lottt ou toeasnenei anlce fosi thpplencerine whamardenineer asd asdhes l c motwelint mnlet  oatueled sr teraglen fooct  nicpinerr cnlcelert c dopsrttott wo yhisellny iol tros all a0 eeysa  f folct neesinelere tr tepark difuritatty ofppus tr teaty iolctinnsid wi ys ouott  t teragrapcerirs perageepine  whems tep anl e pote oncllned iitecti  toated anl eopweu tndiisg\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}